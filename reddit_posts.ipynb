{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb6ede70",
   "metadata": {},
   "source": [
    "# Successful Reddit Posts\n",
    "\n",
    "The file `askscience_data.csv` is a collection of posts from the subreddit r/askscience. The task comes with two parts:\n",
    "\n",
    "1. Determine the attributes of a successful post on r/askscience\n",
    "2. Build a model that can predict the score of a post on r/askscience given at least the title and body of the post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b3a669",
   "metadata": {},
   "source": [
    "## Part 1: Determine the Attributes of A Successful Post\n",
    "Our data comes in a tabular format with the following columns:\n",
    "|id|title|body|tag|datetime|author|score|upvote_ratio|url|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|-|IV|IV|IV|IV|-|T|-|-|\n",
    "\n",
    "The columns `[id, author, url]` are not considered for analysis since they are all identifiers. Perhaps we could use `author` if we had some information on them, which is not the case for this experiment. The column `upvote_ratio` is not also considered as it is highly correlated with the target variable `score` (r = 0.55).\n",
    "\n",
    "To find the attributes that contribute the most to the success of a post, I include the features one by one and examine their effects in improving the models' performance.\n",
    "\n",
    "To annotate the reddit posts as **_successful_**, I use the threshold **score $\\ge$ 100**, which results in a balanced dataset of positive and negative samples.\n",
    "\n",
    "For all the analysis, 80% of the data (randomly chosen) are used for training and the performance of models and features are tested on the remaining 20%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf7c205",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "Previous works have suggested the following features for similar tasks:\n",
    "|Column|Feature|Source|\n",
    "|---|---|---|\n",
    "|title|TF-IDF, BoW|\\[1\\]|\n",
    "|body|TF-IDF, BoW|\\[2\\]|\n",
    "|datetime|hour of day|\\[3\\]|\n",
    "\n",
    "I introduce one-hot-encoding for `tag` column, as there are only 33 unique tags in our dataset.\n",
    "\n",
    "I also show how to learn and use _Embeddings_ for this dataset for a deep neural network (see my notes for _deep neural networks_).\n",
    "\n",
    "...<br>\n",
    "$_{\\text{[1] \"Classifying reddit post titles to subreddit\"}}$<br>\n",
    "$_{\\text{[2] \"What's in the name? Understanding the interplay between titles, content, and communities in social media.\"}}$<br>\n",
    "$_{\\text{[3] \"Widespread underprovision on reddit.\"}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88d522b",
   "metadata": {},
   "source": [
    "### Results\n",
    "Different models were trained and evaluated on the test set (20% randomly chosen). The results are shown below:\n",
    "\n",
    "|Attributes|Model|Accuracy|\n",
    "|---|---|---|\n",
    "|title|Logistic Regression|61%|\n",
    "|title|Random Forest|71%|\n",
    "|title|SVM + RBF kernel|**74%**|\n",
    "|title|Embedding + DN|58%|\n",
    "||||\n",
    "|title + body|Logistic Regression|63%|\n",
    "|title + body|Random Forest|72%|\n",
    "|title + body|SVM + RBF kernel|**73%**|\n",
    "||||\n",
    "|title + tag| Logistic Regression | 63% |\n",
    "|title + tag| Random Forest | **73%** |\n",
    "|title + tag| SVM + RBF kernel | 71%|\n",
    "||||\n",
    "|title + datetime| Logistic Regression | 62% |\n",
    "|title + datetime| Random Forest | **74%** |\n",
    "|title + datetime| SVM + RBF kernel | 59% |\n",
    "||||\n",
    "|title + tag + datetime| Logistic Regression | 65% |\n",
    "|title + tag + datetime| Random Forest | **75%** |\n",
    "|title + tag + datetime| SVM + RBF kernel | 60% |\n",
    "\n",
    "**Conclusion:**\n",
    "* The most important attribute for a successful post is the `title` of the post.\n",
    "* Including `body` added no (or negligible) benefit to the classification task.\n",
    "* `tag` and `datetime` seemed to have positive effects (more obvious for the linear model), but they were not significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fabed0",
   "metadata": {},
   "source": [
    "### Notes\n",
    "There are several areas that could potentially improve the overall quality of this experiment:\n",
    "* I performed hyperparameter tuning for all the mentioned methods (especially, for Random Forest and DN); however, due to time limitation, more thorough hyperparameter tuning were not conducted.\n",
    "* I did a quick research on the features proposed in this domain, but perhaps there were many more that I did not find at the moment. So, I limited the feature engineering aspect to those explained in the _Methodology_ section.\n",
    "* I implemented the **Embedding + Deep Neural Network** just to demonstrate how I would approach this problem in this domain. However, the amount of available data were not sufficient for creating a model from scratch. I believe using a pretrained model for similar tasks, in addition to the network I introduced in this section as the top layer, could achieve a better result. At the moment, I could not find a relevant model to experiment my hypothesis.\n",
    "\n",
    "Please see my codes for this part of the takehome below, after \"Part 2\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b017e75d",
   "metadata": {},
   "source": [
    "## Part 2: Build a Model to Predict Reddit Post Scores\n",
    "For this part of the task, I use the same datasets that I created for the previous part. Since we are predicting a numerical value (`score`), I consider this as a regression problem.\n",
    "\n",
    "There are some outliers in our data (`score` > 20,000) that will have a huge impact on our model's performance. For my analysis, I excluded those samples.\n",
    "\n",
    "The best performance was achieved by Linear Regression model on `title` and `hour` dataset. However, the performance was far from ideal (rMSE=4665). The result was expected since\n",
    "* The number of features were too high for Random Forest.\n",
    "* The features (TF-IDF) had no linear relationship with the target variable.\n",
    "* SVM with RBF kernel calculates similarities between data points; therefore, the dimensionality would be reduced. However, that does not mean that it would find the right \"decision street\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb62692",
   "metadata": {},
   "source": [
    "### Improvements (future works)\n",
    "Some improvements come to my mind:\n",
    "\n",
    "1. As was found in data exploration, almost 40% of our samples have `score` <5. Therefore, a cascaded model could be beneficial for increasing our models' predictions; i.e., a classifier would predict whether a post is _ignored_ or not (`score` < 5), and then the label would be fed to the regression model as an independent variable.\n",
    "2. Although TF-IDF features determine the importance of some words that are helpful for identifying \"succesful\" posts, it does not provide enough information to predict the \"score\" itself. Therefore, better features should be calculated. (I did not find relevant works for this, but my research on this topic was not thorough. Perhaps there are related works that I missed)\n",
    "3. Some works suggested some type of \"sentiment analysis\" as a feature for predicting \"popularity\". Although, those are not exactly aiming for predicting \"score\" values, such feature could be beneficial here as well. That brings me to the point I mentioned for the classification task too. A relevant base model for calculating embeddings/feature vectors, could help in building a neural network that was more suitable for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7d1707",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "115bec3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>tag</th>\n",
       "      <th>datetime</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Post viral cough: why does it get worse after ...</td>\n",
       "      <td>Tl;dr: why is your cough during an upper respi...</td>\n",
       "      <td>Human Body</td>\n",
       "      <td>2022-12-09 02:52:07</td>\n",
       "      <td>CarboniferousCreek</td>\n",
       "      <td>1343.0</td>\n",
       "      <td>0.93</td>\n",
       "      <td>https://www.reddit.com/r/askscience/comments/z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Can an x-ray of an adult show chronic malnouri...</td>\n",
       "      <td>If a person was chronically undernourished thr...</td>\n",
       "      <td>Human Body</td>\n",
       "      <td>2022-12-08 18:28:51</td>\n",
       "      <td>Foxs-In-A-Trenchcoat</td>\n",
       "      <td>426.0</td>\n",
       "      <td>0.91</td>\n",
       "      <td>https://www.reddit.com/r/askscience/comments/z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[META] Bots and AI tools on r/askscience</td>\n",
       "      <td>\\n\\nOver the past few days we have seen a sur...</td>\n",
       "      <td>META</td>\n",
       "      <td>2022-12-08 09:04:25</td>\n",
       "      <td>AskScienceModerator</td>\n",
       "      <td>2218.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>https://www.reddit.com/r/askscience/comments/z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Can you predictably manipulate a magnetic gas?</td>\n",
       "      <td>Does there exist a gas that changes in respons...</td>\n",
       "      <td>Physics</td>\n",
       "      <td>2022-12-08 20:01:53</td>\n",
       "      <td>hufsa7</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>https://www.reddit.com/r/askscience/comments/z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Are there cells in humans (or other mammals) t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Medicine</td>\n",
       "      <td>2022-12-09 01:11:34</td>\n",
       "      <td>Velym</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.89</td>\n",
       "      <td>https://www.reddit.com/r/askscience/comments/z...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           0  Post viral cough: why does it get worse after ...   \n",
       "1           1  Can an x-ray of an adult show chronic malnouri...   \n",
       "2           2           [META] Bots and AI tools on r/askscience   \n",
       "3           3     Can you predictably manipulate a magnetic gas?   \n",
       "4           4  Are there cells in humans (or other mammals) t...   \n",
       "\n",
       "                                                body         tag  \\\n",
       "0  Tl;dr: why is your cough during an upper respi...  Human Body   \n",
       "1  If a person was chronically undernourished thr...  Human Body   \n",
       "2   \\n\\nOver the past few days we have seen a sur...        META   \n",
       "3  Does there exist a gas that changes in respons...     Physics   \n",
       "4                                                NaN    Medicine   \n",
       "\n",
       "              datetime                author   score  upvote_ratio  \\\n",
       "0  2022-12-09 02:52:07    CarboniferousCreek  1343.0          0.93   \n",
       "1  2022-12-08 18:28:51  Foxs-In-A-Trenchcoat   426.0          0.91   \n",
       "2  2022-12-08 09:04:25   AskScienceModerator  2218.0          0.92   \n",
       "3  2022-12-08 20:01:53                hufsa7    48.0          0.83   \n",
       "4  2022-12-09 01:11:34                 Velym    14.0          0.89   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.reddit.com/r/askscience/comments/z...  \n",
       "1  https://www.reddit.com/r/askscience/comments/z...  \n",
       "2  https://www.reddit.com/r/askscience/comments/z...  \n",
       "3  https://www.reddit.com/r/askscience/comments/z...  \n",
       "4  https://www.reddit.com/r/askscience/comments/z...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('askscience_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6203fd93",
   "metadata": {},
   "source": [
    "#### Split the data into training and test sets\n",
    "I set aside 20% of data (randomly chosen) as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "205664a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size: 3364\tvs\ttest set size: 841\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(42)\n",
    "\n",
    "train_df_raw, test_df_raw = train_test_split(df, test_size=0.2, random_state=42)\n",
    "print(f'training set size: {train_df_raw.shape[0]}\\tvs\\ttest set size: {test_df_raw.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4077d5",
   "metadata": {},
   "source": [
    "#### Data exploration\n",
    "Let's check the correlation between `upvote_ratio` and `score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5885bf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a 0.5501576124022011 correlation between upvote_ratio and score.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "r = np.corrcoef(train_df_raw.upvote_ratio, train_df_raw.score)\n",
    "print(f'There is a {r[0][1]} correlation between upvote_ratio and score.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f2bd0d",
   "metadata": {},
   "source": [
    "Now, let's look at the distribution of `score` values for our samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18cae317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df_raw.score.hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b9298d",
   "metadata": {},
   "source": [
    "Almost half of the data have `score` $\\le$ 100. There are also ~50 samples that have a very high `score` value (>20,000), and should be considered as outliers for the regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689da3a1",
   "metadata": {},
   "source": [
    "#### Data annotation and feature engineering\n",
    "* We define positive class as `score > 100`.\n",
    "* We calculate the following features:\n",
    "    * TF-IDF representation for title.\n",
    "    * TF-IDF representation for title and body (concatenated).\n",
    "    * One-hot-encoding for tag.\n",
    "    * Hour from datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "97d73319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data are annotated based on the score values.\n"
     ]
    }
   ],
   "source": [
    "train_df_raw['successful'] = np.where(train_df_raw['score'] > 100, True, False)\n",
    "test_df_raw['successful'] = np.where(test_df_raw['score'] > 100, True, False)\n",
    "print(\"Data are annotated based on the score values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10827efc",
   "metadata": {},
   "source": [
    "First, we create a dataset considering only the title of posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8bb1fba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/matin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/matin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def removeStopWords(texts):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    clean_texts = []\n",
    "    for i in range(len(texts)):\n",
    "        text = texts[i]\n",
    "        clean_text = re.sub('[^a-zA-Z]', ' ', str(text))\n",
    "        clean_text = clean_text.lower()\n",
    "        clean_text = clean_text.split()\n",
    "        clean_text = [lemmatizer.lemmatize(word) for word in clean_text if word not in set(stopwords)]\n",
    "        clean_text = ' '.join(clean_text)\n",
    "        clean_texts.append(clean_text)\n",
    "\n",
    "    return clean_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2b2585aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_titles = removeStopWords(train_df_raw.title.values)\n",
    "test_titles = removeStopWords(test_df_raw.title.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "45c972d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "tf_idf_vectorizer.fit(train_titles)\n",
    "\n",
    "train_title_tfidf = tf_idf_vectorizer.transform(train_titles)\n",
    "test_title_tfidf = tf_idf_vectorizer.transform(test_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "db72e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['tfidf_{}'.format(i) for i in range(train_title_tfidf.shape[1])]\n",
    "train_title_df = pd.DataFrame(train_title_tfidf.toarray(), columns=column_names)\n",
    "train_title_df['successful'] = train_df_raw.successful.values\n",
    "test_title_df = pd.DataFrame(test_title_tfidf.toarray(), columns=column_names)\n",
    "test_title_df['successful'] = test_df_raw.successful.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9f80d6",
   "metadata": {},
   "source": [
    "Next, we create a dataset for title and body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "960aafd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenateTitleAndBody(titles, bodies):\n",
    "    result = []\n",
    "    for i in range(len(titles)):\n",
    "        title = titles[i]\n",
    "        body = bodies[i]\n",
    "        if str(body) == 'nan':\n",
    "            result.append(title)\n",
    "            continue\n",
    "        result.append(title + ' ' + body)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bc7fb64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_titles_bodies = removeStopWords(concatenateTitleAndBody(train_df_raw.title.values, train_df_raw.body.values))\n",
    "test_titles_bodies = removeStopWords(concatenateTitleAndBody(test_df_raw.title.values, test_df_raw.body.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8aad1fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "tf_idf_vectorizer.fit(train_titles_bodies)\n",
    "\n",
    "train_titlebody_tfidf = tf_idf_vectorizer.transform(train_titles_bodies)\n",
    "test_titlebody_tfidf = tf_idf_vectorizer.transform(test_titles_bodies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6e19b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['tfidf_{}'.format(i) for i in range(train_titlebody_tfidf.shape[1])]\n",
    "train_titlebody_df = pd.DataFrame(train_titlebody_tfidf.toarray(), columns=column_names)\n",
    "train_titlebody_df['successful'] = train_df_raw.successful.values\n",
    "test_titlebody_df = pd.DataFrame(test_titlebody_tfidf.toarray(), columns=column_names)\n",
    "test_titlebody_df['successful'] = test_df_raw.successful.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b809e038",
   "metadata": {},
   "source": [
    "Next, we create a dataset considering titles and tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bddfe97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(dtype=np.int16, handle_unknown='infrequent_if_exist')\n",
    "one_hot_encoder.fit(train_df_raw.tag.values.reshape(-1, 1))\n",
    "\n",
    "training_tag_encodings = one_hot_encoder.transform(train_df_raw.tag.values.reshape(-1, 1))\n",
    "tag_columns = [\"tag_{}\".format(i) for i in range(training_tag_encodings.shape[1])]\n",
    "training_tags_df = pd.DataFrame(training_tag_encodings.toarray(), columns=tag_columns)\n",
    "\n",
    "test_tag_encodings = one_hot_encoder.transform(test_df_raw.tag.values.reshape(-1, 1))\n",
    "test_tags_df = pd.DataFrame(test_tag_encodings.toarray(), columns=tag_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b21477be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_titletag_df = pd.concat([train_title_df, training_tags_df], axis=1)\n",
    "test_titletag_df = pd.concat([test_title_df, test_tags_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dcbace",
   "metadata": {},
   "source": [
    "Next, we create a dataset from `title` and `datetime` attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ae05a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "def timeOfDay(date_and_times):\n",
    "    time_of_days = []\n",
    "    for date_and_time in date_and_times:\n",
    "        datetime_obj = datetime.strptime(date_and_time, \"%Y-%m-%d %H:%M:%S\")\n",
    "        time_of_days.append(datetime_obj.hour)\n",
    "        \n",
    "    return time_of_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e98887ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_titlehour_df = train_title_df.copy()\n",
    "train_titlehour_df['hour'] = timeOfDay(train_df_raw.datetime.values)\n",
    "test_titlehour_df = test_title_df.copy()\n",
    "test_titlehour_df['hour'] = timeOfDay(test_df_raw.datetime.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce81cfb1",
   "metadata": {},
   "source": [
    "Finally, we create a dataset with `title`, `tag`, and `hour`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0cbc3a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_titletaghour_df = train_titletag_df.copy()\n",
    "train_titletaghour_df['hour'] = timeOfDay(train_df_raw.datetime.values)\n",
    "test_titletaghour_df = test_titletag_df.copy()\n",
    "test_titletaghour_df['hour'] = timeOfDay(test_df_raw.datetime.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d711812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since some posts do not have tags, we have to drop data points with NaN values.\n",
    "train_titletag_df.dropna(inplace=True)\n",
    "train_titletaghour_df.dropna(inplace=True)\n",
    "test_titletag_df.dropna(inplace=True)\n",
    "test_titletaghour_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7d4c70",
   "metadata": {},
   "source": [
    "## Evaluations For Part 1 (Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8dc4c3",
   "metadata": {},
   "source": [
    "#### Evaluation on title-only dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "590e6ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.6777645659928656\n",
      "Accuracy on test set: 0.6099881093935791\n"
     ]
    }
   ],
   "source": [
    "# Model trainings\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# We have more features than the number of samples. Therefore, we use 'l1' regularization\n",
    "# to reduce the number of features invovled in the decision making.\n",
    "log_clf = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "log_clf.fit(train_title_df.drop(['successful'], axis=1), train_title_df.successful)\n",
    "\n",
    "# Evaluations\n",
    "y_train_pred = log_clf.predict(train_title_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on training set: {accuracy_score(train_title_df.successful.values, y_train_pred)}')\n",
    "y_test_pred = log_clf.predict(test_title_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on test set: {accuracy_score(test_title_df.successful.values, y_test_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "35ad73b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 1.0\n",
      "Accuracy on test set: 0.7110582639714625\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(train_title_df.drop(['successful'], axis=1), train_title_df.successful)\n",
    "\n",
    "# Evaluations\n",
    "y_train_pred = rf_clf.predict(train_title_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on training set: {accuracy_score(train_title_df.successful.values, y_train_pred)}')\n",
    "y_test_pred = rf_clf.predict(test_title_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on test set: {accuracy_score(test_title_df.successful.values, y_test_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a055109c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.9892984542211652\n",
      "Accuracy on test set: 0.7395957193816884\n"
     ]
    }
   ],
   "source": [
    "svc_clf = SVC(kernel='rbf')\n",
    "svc_clf.fit(train_title_df.drop(['successful'], axis=1), train_title_df.successful)\n",
    "\n",
    "# Evaluations\n",
    "y_train_pred = svc_clf.predict(train_title_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on training set: {accuracy_score(train_title_df.successful.values, y_train_pred)}')\n",
    "y_test_pred = svc_clf.predict(test_title_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on test set: {accuracy_score(test_title_df.successful.values, y_test_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ae749c",
   "metadata": {},
   "source": [
    "##### Deep neural network with embeddings\n",
    "This part is for demonstration of how a deep neural network can be built using an embedding layer for feature calculations.\n",
    "\n",
    "The important part is showing how to build a vocabulary and feed it to neural network model.\n",
    "\n",
    "The model here is built from scratch; we need many more data points for it to learn an efficient embedding. The better approach is to use a pretrained model as the base, and add this model as the top layer. Then we could use the dataset we have to fine-tune the top layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d7ed365a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('would', 293), ('like', 190), ('human', 170), ('earth', 167), ('get', 167)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building a vocoabulary from titles.\n",
    "from collections import Counter\n",
    "\n",
    "vocabulary = Counter()\n",
    "for text in train_titles:\n",
    "    for word in text.split(' '):\n",
    "        vocabulary.update([word])\n",
    "\n",
    "vocabulary.most_common()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec6c831d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5), dtype=int64, numpy=array([[6860, 6435, 7054,  554,  107]])>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "vocab_size = 10000\n",
    "num_oov_buckets = 1000\n",
    "\n",
    "truncated_vocabulary = [word for word, count in vocabulary.most_common()[:vocab_size]]\n",
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)\n",
    "\n",
    "table.lookup(tf.constant([b'This is a great test'.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ec31c0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeText(text, sentence_length=25):\n",
    "    words = text.split()\n",
    "    if len(words) < sentence_length:\n",
    "        words = words + ['<pad>'] * (sentence_length - len(words))\n",
    "    return table.lookup(tf.constant([words[:sentence_length]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b72c6b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 128)         1408000   \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 512)         66048     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 512)         0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, None, 256)         131328    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, None, 128)         32896     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, None, 1)           129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,638,401\n",
      "Trainable params: 1,638,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "embed_size = 128\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size, input_shape=[None]),\n",
    "    keras.layers.Dense(512, activation='selu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(256, activation='selu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(128, activation='selu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7308a561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 14:30:43.768275: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 1s 8ms/step - loss: 0.6926 - accuracy: 0.5490\n",
      "Epoch 2/5\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.6821 - accuracy: 0.5837\n",
      "Epoch 3/5\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.6702 - accuracy: 0.6036\n",
      "Epoch 4/5\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.6615 - accuracy: 0.6112\n",
      "Epoch 5/5\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.6583 - accuracy: 0.6085\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "X_train_encoded = tf.convert_to_tensor(np.array([encodeText(text) for text in train_titles]).squeeze())\n",
    "y_train_encoded = np.asarray(train_title_df.successful.values).astype('float32').reshape((-1,1))\n",
    "history = model.fit(X_train_encoded, y_train_encoded, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df1d7ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6929308772087097, 0.5817835330963135]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_encoded = tf.convert_to_tensor(np.array([encodeText(text) for text in test_titles]).squeeze())\n",
    "y_test_encoded = np.asarray(test_title_df.successful.values).astype('float32').reshape((-1,1))\n",
    "\n",
    "model.evaluate(X_test_encoded, y_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b13671b",
   "metadata": {},
   "source": [
    "As was expected, the DN model does not achieve high accuracy on either training set or test set. That means the model is underfitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6653dd",
   "metadata": {},
   "source": [
    "#### Evaluation on {title and body} dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6585be5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.6917360285374554\n",
      "Accuracy on test set: 0.6278240190249703\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression\")\n",
    "log_clf = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "log_clf.fit(train_titlebody_df.drop(['successful'], axis=1), train_titlebody_df.successful)\n",
    "\n",
    "# Evaluations\n",
    "y_train_pred = log_clf.predict(train_titlebody_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on training set: {accuracy_score(train_titlebody_df.successful.values, y_train_pred)}')\n",
    "y_test_pred = log_clf.predict(test_titlebody_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on test set: {accuracy_score(test_titlebody_df.successful.values, y_test_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "63aa54ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Accuracy on training set: 1.0\n",
      "Accuracy on test set: 0.7217598097502973\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest\")\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(train_titlebody_df.drop(['successful'], axis=1), train_titlebody_df.successful)\n",
    "\n",
    "# Evaluations\n",
    "y_train_pred = rf_clf.predict(train_titlebody_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on training set: {accuracy_score(train_titlebody_df.successful.values, y_train_pred)}')\n",
    "y_test_pred = rf_clf.predict(test_titlebody_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on test set: {accuracy_score(test_titlebody_df.successful.values, y_test_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "596115b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM + RBF kernel\n",
      "Accuracy on training set: 0.9907847800237812\n",
      "Accuracy on test set: 0.7288941736028538\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM + RBF kernel\")\n",
    "svc_clf = SVC(kernel='rbf')\n",
    "svc_clf.fit(train_titlebody_df.drop(['successful'], axis=1), train_titlebody_df.successful)\n",
    "\n",
    "# Evaluations\n",
    "y_train_pred = svc_clf.predict(train_titlebody_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on training set: {accuracy_score(train_titlebody_df.successful.values, y_train_pred)}')\n",
    "y_test_pred = svc_clf.predict(test_titlebody_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on test set: {accuracy_score(test_titlebody_df.successful.values, y_test_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05052d01",
   "metadata": {},
   "source": [
    "#### Evaluation on {title and tag} dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bb57e9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy on training set: 0.6932223543400713\n",
      "Accuracy on test set: 0.633769322235434\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression\")\n",
    "log_clf = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "log_clf.fit(train_titletag_df.drop(['successful'], axis=1), train_titletag_df.successful)\n",
    "\n",
    "# Evaluations\n",
    "y_train_pred = log_clf.predict(train_titletag_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on training set: {accuracy_score(train_titletag_df.successful.values, y_train_pred)}')\n",
    "y_test_pred = log_clf.predict(test_titletag_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on test set: {accuracy_score(test_titletag_df.successful.values, y_test_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7e2707a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Accuracy on training set: 1.0\n",
      "Accuracy on test set: 0.7288941736028538\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest\")\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(train_titletag_df.drop(['successful'], axis=1), train_titletag_df.successful)\n",
    "\n",
    "# Evaluations\n",
    "y_train_pred = rf_clf.predict(train_titletag_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on training set: {accuracy_score(train_titletag_df.successful.values, y_train_pred)}')\n",
    "y_test_pred = rf_clf.predict(test_titletag_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on test set: {accuracy_score(test_titletag_df.successful.values, y_test_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e42ad801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM + RBF kernel\n",
      "Accuracy on training set: 0.9530321046373365\n",
      "Accuracy on test set: 0.7063020214030915\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM + RBF kernel\")\n",
    "svc_clf = SVC(kernel='rbf')\n",
    "svc_clf.fit(train_titletag_df.drop(['successful'], axis=1), train_titletag_df.successful)\n",
    "\n",
    "# Evaluations\n",
    "y_train_pred = svc_clf.predict(train_titletag_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on training set: {accuracy_score(train_titletag_df.successful.values, y_train_pred)}')\n",
    "y_test_pred = svc_clf.predict(test_titletag_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on test set: {accuracy_score(test_titletag_df.successful.values, y_test_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe99865",
   "metadata": {},
   "source": [
    "#### Evaluation on {title and hour} dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e46ed8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy on training set: 0.6843043995243757\n",
      "Accuracy on test set: 0.6254458977407847\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression\")\n",
    "log_clf = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "log_clf.fit(train_titlehour_df.drop(['successful'], axis=1), train_titlehour_df.successful)\n",
    "\n",
    "# Evaluations\n",
    "y_train_pred = log_clf.predict(train_titlehour_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on training set: {accuracy_score(train_titlehour_df.successful.values, y_train_pred)}')\n",
    "y_test_pred = log_clf.predict(test_titlehour_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on test set: {accuracy_score(test_titlehour_df.successful.values, y_test_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e7181212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Accuracy on training set: 1.0\n",
      "Accuracy on test set: 0.7360285374554102\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest\")\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(train_titlehour_df.drop(['successful'], axis=1), train_titlehour_df.successful)\n",
    "\n",
    "# Evaluations\n",
    "y_train_pred = rf_clf.predict(train_titlehour_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on training set: {accuracy_score(train_titlehour_df.successful.values, y_train_pred)}')\n",
    "y_test_pred = rf_clf.predict(test_titlehour_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on test set: {accuracy_score(test_titlehour_df.successful.values, y_test_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3ca199cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM + RBF kernel\n",
      "Accuracy on training set: 0.6019619500594531\n",
      "Accuracy on test set: 0.5897740784780023\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM + RBF kernel\")\n",
    "svc_clf = SVC(kernel='rbf')\n",
    "svc_clf.fit(train_titlehour_df.drop(['successful'], axis=1), train_titlehour_df.successful)\n",
    "\n",
    "# Evaluations\n",
    "y_train_pred = svc_clf.predict(train_titlehour_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on training set: {accuracy_score(train_titlehour_df.successful.values, y_train_pred)}')\n",
    "y_test_pred = svc_clf.predict(test_titlehour_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on test set: {accuracy_score(test_titlehour_df.successful.values, y_test_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f5330e",
   "metadata": {},
   "source": [
    "#### Evaluation on {title, tag, and hour} dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "28706cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy on training set: 0.6970868014268727\n",
      "Accuracy on test set: 0.6504161712247325\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression\")\n",
    "log_clf = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "log_clf.fit(train_titletaghour_df.drop(['successful'], axis=1), train_titletaghour_df.successful)\n",
    "\n",
    "# Evaluations\n",
    "y_train_pred = log_clf.predict(train_titletaghour_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on training set: {accuracy_score(train_titletaghour_df.successful.values, y_train_pred)}')\n",
    "y_test_pred = log_clf.predict(test_titletaghour_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on test set: {accuracy_score(test_titletaghour_df.successful.values, y_test_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "94f5a1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Accuracy on training set: 1.0\n",
      "Accuracy on test set: 0.7502972651605232\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest\")\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(train_titletaghour_df.drop(['successful'], axis=1), train_titletaghour_df.successful)\n",
    "\n",
    "# Evaluations\n",
    "y_train_pred = rf_clf.predict(train_titletaghour_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on training set: {accuracy_score(train_titletaghour_df.successful.values, y_train_pred)}')\n",
    "y_test_pred = rf_clf.predict(test_titletaghour_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on test set: {accuracy_score(test_titletaghour_df.successful.values, y_test_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "09fdd87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM + RBF kernel\n",
      "Accuracy on training set: 0.6111771700356718\n",
      "Accuracy on test set: 0.6016646848989299\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM + RBF kernel\")\n",
    "svc_clf = SVC(kernel='rbf')\n",
    "svc_clf.fit(train_titletaghour_df.drop(['successful'], axis=1), train_titletaghour_df.successful)\n",
    "\n",
    "# Evaluations\n",
    "y_train_pred = svc_clf.predict(train_titletaghour_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on training set: {accuracy_score(train_titletaghour_df.successful.values, y_train_pred)}')\n",
    "y_test_pred = svc_clf.predict(test_titletaghour_df.drop(['successful'], axis=1))\n",
    "print(f'Accuracy on test set: {accuracy_score(test_titletaghour_df.successful.values, y_test_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24958639",
   "metadata": {},
   "source": [
    "## Evaluations For Part 2 (Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4758de8",
   "metadata": {},
   "source": [
    "#### Preparing datasets for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e4701673",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_title_df = train_title_df.drop(['successful'], axis=1)\n",
    "train_title_df['score'] = train_df_raw.score.values\n",
    "test_title_df = test_title_df.drop(['successful'], axis=1)\n",
    "test_title_df['score'] = test_df_raw.score.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dbcfd19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_titlebody_df.drop(['successful'], axis=1, inplace=True)\n",
    "train_titlebody_df['score'] = train_df_raw.score.values\n",
    "test_titlebody_df.drop(['successful'], axis=1, inplace=True)\n",
    "test_titlebody_df['score'] = test_df_raw.score.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2dc388d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_titletag_df.drop(['successful'], axis=1, inplace=True)\n",
    "train_titletag_df['score'] = train_df_raw.score.values\n",
    "test_titletag_df.drop(['successful'], axis=1, inplace=True)\n",
    "test_titletag_df['score'] = test_df_raw.score.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "496e260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_titlehour_df.drop(['successful'], axis=1, inplace=True)\n",
    "train_titlehour_df['score'] = train_df_raw.score.values\n",
    "test_titlehour_df.drop(['successful'], axis=1, inplace=True)\n",
    "test_titlehour_df['score'] = test_df_raw.score.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c5ab68e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_titletaghour_df.drop(['successful'], axis=1, inplace=True)\n",
    "train_titletaghour_df['score'] = train_df_raw.score.values\n",
    "test_titletaghour_df.drop(['successful'], axis=1, inplace=True)\n",
    "test_titletaghour_df['score'] = test_df_raw.score.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "eb16235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since some posts do not have tags, we have to drop data points with NaN values.\n",
    "train_titletag_df.dropna(inplace=True)\n",
    "train_titletaghour_df.dropna(inplace=True)\n",
    "test_titletag_df.dropna(inplace=True)\n",
    "test_titletaghour_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97bb57e",
   "metadata": {},
   "source": [
    "### Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21ef18a",
   "metadata": {},
   "source": [
    "#### Evaluation on {title + body} dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2cb8e91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression with L1 regularization\n",
      "rMSE on training set: 3220.4854077050695\n",
      "rMSE on test set: 5192.082642790545\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"Linear regression with L1 regularization\")\n",
    "lin_reg = Lasso()\n",
    "lin_reg.fit(train_titlebody_df.drop(['score'], axis=1), train_titlebody_df.score.values)\n",
    "\n",
    "y_pred = lin_reg.predict(train_titlebody_df.drop(['score'], axis=1))\n",
    "print(\"rMSE on training set: {}\".format(mean_squared_error(train_titlebody_df.score.values, y_pred, squared=False)))\n",
    "y_pred = lin_reg.predict(test_titlebody_df.drop(['score'], axis=1))\n",
    "print(\"rMSE on test set: {}\".format(mean_squared_error(test_titlebody_df.score.values, y_pred, squared=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cb403e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "rMSE on training set: 1674.2776745600809\n",
      "rMSE on test set: 4685.70120377921\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "print(\"Random Forest\")\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, min_samples_split=2, max_features=1.0, random_state=42)\n",
    "rf_reg.fit(train_titlebody_df.drop(['score'], axis=1), train_titlebody_df.score.values)\n",
    "\n",
    "y_pred = rf_reg.predict(train_titlebody_df.drop(['score'], axis=1))\n",
    "print(\"rMSE on training set: {}\".format(mean_squared_error(train_titlebody_df.score.values, y_pred, squared=False)))\n",
    "y_pred = rf_reg.predict(test_titlebody_df.drop(['score'], axis=1))\n",
    "print(\"rMSE on test set: {}\".format(mean_squared_error(test_titlebody_df.score.values, y_pred, squared=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bf1194f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM + RBF kernel\n",
      "rMSE on training set: 6601.498275472558\n",
      "rMSE on test set: 7030.440099895105\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "print(\"SVM + RBF kernel\")\n",
    "svr = SVR(kernel='rbf')\n",
    "svr.fit(train_titlebody_df.drop(['score'], axis=1), train_titlebody_df.score.values)\n",
    "\n",
    "y_pred = svr.predict(train_titlebody_df.drop(['score'], axis=1))\n",
    "print(\"rMSE on training set: {}\".format(mean_squared_error(train_titlebody_df.score.values, y_pred, squared=False)))\n",
    "y_pred = svr.predict(test_titlebody_df.drop(['score'], axis=1))\n",
    "print(\"rMSE on test set: {}\".format(mean_squared_error(test_titlebody_df.score.values, y_pred, squared=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d3a71e",
   "metadata": {},
   "source": [
    "#### Evaluation on {title + tag}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0000e880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression with L1 regularization\n",
      "rMSE on training set: 3135.507634872323\n",
      "rMSE on test set: 5426.111290488777\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear regression with L1 regularization\")\n",
    "lin_reg = Lasso()\n",
    "lin_reg.fit(train_titletag_df.drop(['score'], axis=1), train_titletag_df.score.values)\n",
    "\n",
    "y_pred = lin_reg.predict(train_titletag_df.drop(['score'], axis=1))\n",
    "print(\"rMSE on training set: {}\".format(mean_squared_error(train_titletag_df.score.values, y_pred, squared=False)))\n",
    "y_pred = lin_reg.predict(test_titletag_df.drop(['score'], axis=1))\n",
    "print(\"rMSE on test set: {}\".format(mean_squared_error(test_titletag_df.score.values, y_pred, squared=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "35ec7996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "rMSE on training set: 1663.8305016912539\n",
      "rMSE on test set: 4672.430810661432\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest\")\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, min_samples_split=2, max_features=1.0, random_state=42)\n",
    "rf_reg.fit(train_titletag_df.drop(['score'], axis=1), train_titletag_df.score.values)\n",
    "\n",
    "y_pred = rf_reg.predict(train_titletag_df.drop(['score'], axis=1))\n",
    "print(\"rMSE on training set: {}\".format(mean_squared_error(train_titletag_df.score.values, y_pred, squared=False)))\n",
    "y_pred = rf_reg.predict(test_titletag_df.drop(['score'], axis=1))\n",
    "print(\"rMSE on test set: {}\".format(mean_squared_error(test_titletag_df.score.values, y_pred, squared=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "954eb25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM + RBF kernel\n",
      "rMSE on training set: 6598.622580577136\n",
      "rMSE on test set: 7027.517228162477\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM + RBF kernel\")\n",
    "svr = SVR(kernel='rbf')\n",
    "svr.fit(train_titletag_df.drop(['score'], axis=1), train_titletag_df.score.values)\n",
    "\n",
    "y_pred = svr.predict(train_titletag_df.drop(['score'], axis=1))\n",
    "print(\"rMSE on training set: {}\".format(mean_squared_error(train_titletag_df.score.values, y_pred, squared=False)))\n",
    "y_pred = svr.predict(test_titletag_df.drop(['score'], axis=1))\n",
    "print(\"rMSE on test set: {}\".format(mean_squared_error(test_titletag_df.score.values, y_pred, squared=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff15ab1c",
   "metadata": {},
   "source": [
    "#### Evaluation on {title + hour}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7d445b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression with L1 regularization\n",
      "rMSE on training set: 3158.9485825313136\n",
      "rMSE on test set: 5483.62856628311\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear regression with L1 regularization\")\n",
    "lin_reg = Lasso()\n",
    "lin_reg.fit(train_titlehour_df.drop(['score'], axis=1), train_titlehour_df.score.values)\n",
    "\n",
    "y_pred = lin_reg.predict(train_titlehour_df.drop(['score'], axis=1))\n",
    "print(\"rMSE on training set: {}\".format(mean_squared_error(train_titlehour_df.score.values, y_pred, squared=False)))\n",
    "y_pred = lin_reg.predict(test_titlehour_df.drop(['score'], axis=1))\n",
    "print(\"rMSE on test set: {}\".format(mean_squared_error(test_titlehour_df.score.values, y_pred, squared=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "18e7c212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "rMSE on training set: 1681.960758843889\n",
      "rMSE on test set: 4665.054585659561\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest\")\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, min_samples_split=2, max_features=1.0, random_state=42)\n",
    "rf_reg.fit(train_titlehour_df.drop(['score'], axis=1), train_titlehour_df.score.values)\n",
    "\n",
    "y_pred = rf_reg.predict(train_titlehour_df.drop(['score'], axis=1))\n",
    "print(\"rMSE on training set: {}\".format(mean_squared_error(train_titlehour_df.score.values, y_pred, squared=False)))\n",
    "y_pred = rf_reg.predict(test_titlehour_df.drop(['score'], axis=1))\n",
    "print(\"rMSE on test set: {}\".format(mean_squared_error(test_titlehour_df.score.values, y_pred, squared=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e790f9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM + RBF kernel\n",
      "rMSE on training set: 6583.743024323937\n",
      "rMSE on test set: 7011.133369271167\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM + RBF kernel\")\n",
    "svr = SVR(kernel='rbf')\n",
    "svr.fit(train_titlehour_df.drop(['score'], axis=1), train_titlehour_df.score.values)\n",
    "\n",
    "y_pred = svr.predict(train_titlehour_df.drop(['score'], axis=1))\n",
    "print(\"rMSE on training set: {}\".format(mean_squared_error(train_titlehour_df.score.values, y_pred, squared=False)))\n",
    "y_pred = svr.predict(test_titlehour_df.drop(['score'], axis=1))\n",
    "print(\"rMSE on test set: {}\".format(mean_squared_error(test_titlehour_df.score.values, y_pred, squared=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7330ea5e",
   "metadata": {},
   "source": [
    "#### Evaluation on {title + tag + hour}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "806d8877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression with L1 regularization\n",
      "rMSE on training set: 3125.865106328158\n",
      "rMSE on test set: 5433.500187242344\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear regression with L1 regularization\")\n",
    "lin_reg = Lasso()\n",
    "lin_reg.fit(train_titletaghour_df.drop(['score'], axis=1), train_titletaghour_df.score.values)\n",
    "\n",
    "y_pred = lin_reg.predict(train_titletaghour_df.drop(['score'], axis=1))\n",
    "print(\"rMSE on training set: {}\".format(mean_squared_error(train_titletaghour_df.score.values, y_pred, squared=False)))\n",
    "y_pred = lin_reg.predict(test_titletaghour_df.drop(['score'], axis=1))\n",
    "print(\"rMSE on test set: {}\".format(mean_squared_error(test_titletaghour_df.score.values, y_pred, squared=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3b24fdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "rMSE on training set: 1653.7440846398833\n",
      "rMSE on test set: 4729.288976115762\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest\")\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, min_samples_split=2, max_features=1.0, random_state=42)\n",
    "rf_reg.fit(train_titletaghour_df.drop(['score'], axis=1), train_titletaghour_df.score.values)\n",
    "\n",
    "y_pred = rf_reg.predict(train_titletaghour_df.drop(['score'], axis=1))\n",
    "print(\"rMSE on training set: {}\".format(mean_squared_error(train_titletaghour_df.score.values, y_pred, squared=False)))\n",
    "y_pred = rf_reg.predict(test_titletaghour_df.drop(['score'], axis=1))\n",
    "print(\"rMSE on test set: {}\".format(mean_squared_error(test_titletaghour_df.score.values, y_pred, squared=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b1276977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM + RBF kernel\n",
      "rMSE on training set: 6583.786410928867\n",
      "rMSE on test set: 7011.186127324637\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM + RBF kernel\")\n",
    "svr = SVR(kernel='rbf')\n",
    "svr.fit(train_titletaghour_df.drop(['score'], axis=1), train_titletaghour_df.score.values)\n",
    "\n",
    "y_pred = svr.predict(train_titletaghour_df.drop(['score'], axis=1))\n",
    "print(\"rMSE on training set: {}\".format(mean_squared_error(train_titletaghour_df.score.values, y_pred, squared=False)))\n",
    "y_pred = svr.predict(test_titletaghour_df.drop(['score'], axis=1))\n",
    "print(\"rMSE on test set: {}\".format(mean_squared_error(test_titletaghour_df.score.values, y_pred, squared=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5771ba26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
